{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task 2 Generate Sparse Representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import re\n",
    "from collections import Counter  \n",
    "from bs4 import BeautifulSoup as bsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the files for used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = open('./stopwords_en.txt','r',encoding = 'windows-1252')\n",
    "sword = stop.read()\n",
    "tokenizer1 = RegexpTokenizer(r\"\\s+\", gaps=True)\n",
    "tokens1 = tokenizer1.tokenize(sword)\n",
    "stopword = set(tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make tokenization for the text and remove the stopped word from the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = \"./txt_files\"\n",
    "files= os.listdir(txt_file_path) \n",
    "dic = {}\n",
    "blist = []\n",
    "for file in files:\n",
    "    txt = open(txt_file_path+\"/\"+file,'r',encoding = 'windows-1252')\n",
    "    text = txt.read()\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\", gaps=False)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stopped_tokens = [w for w in tokens if w not in stopword]\n",
    "    #print(len(stopped_tokens))\n",
    "    alist = list(set(stopped_tokens))\n",
    "    #print(alist)\n",
    "    for j in range(len(alist)):\n",
    "        blist.append(alist[j])\n",
    "\n",
    "#use counter function to count the number of the words in the list       \n",
    "freq_counts = Counter(blist)\n",
    "dic = dict(freq_counts)\n",
    "dic1 = {word:j for word, j in dic.items() if j <= 132}\n",
    "newlist = list(dic1)\n",
    "newlist.sort()\n",
    "data_index = {data:str(k) for k,data in enumerate(newlist)}\n",
    "\n",
    "#output the result to the txt file\n",
    "content = ''\n",
    "file = open(\"./vocab.txt\", \"w+\")\n",
    "for data, index in data_index.items():\n",
    "    content = content + data + \":\" + index + '\\n'\n",
    "file.write(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2.2 topic_seg.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: following the requirement from the moodle:\n",
    "##### \"The aim of this task is to build sparse representations for the meeting transcripts generated in task 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the boundary of the text files and make the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_file_path = \"./txt_files\"\n",
    "files= os.listdir(txt_file_path) \n",
    "marker = ''\n",
    "for file in files:\n",
    "    txt = open(txt_file_path+\"/\"+file,'r',encoding = 'windows-1252')\n",
    "    text = txt.read()\n",
    "    zlist = re.split(r'\\n',text)[:-1]\n",
    "    vlist = []\n",
    "    name = re.search(r'(\\w+)(\\.txt)',file).group(1)\n",
    "    for i, sep in enumerate(zlist):\n",
    "        #print(i,sep)\n",
    "        if sep != \"**********\":\n",
    "            vlist.append(0)\n",
    "        if sep == \"**********\":\n",
    "            vlist.append(1)\n",
    "    #print(vlist)\n",
    "    marker = marker + (name + ':' + str(vlist)[1:-1]) + '\\n'\n",
    "    marker1 = marker.replace(\"  \",\" \")\n",
    "f = open(\"./topic_segs.txt\", \"w+\")\n",
    "f.write(marker1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 ./sparse_files/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse each file into the index and occurance format to txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the vocaburary list above\n",
    "word_dic = data_index.copy()\n",
    "#word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_file_path = \"./txt_files\"\n",
    "files= os.listdir(txt_file_path)\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\", gaps=False)\n",
    "for file in files:\n",
    "    txt = open(txt_file_path+\"/\"+file,'r',encoding = 'windows-1252')\n",
    "    text = txt.read()\n",
    "    xlist = re.split(r\"\\n\", text.lower())\n",
    "    ylist = []\n",
    "    for char in xlist:\n",
    "        tokens = tokenizer.tokenize(char)\n",
    "        #print(tokens)\n",
    "        stopped_tokens = [w for w in tokens if w not in stopword]\n",
    "        #print(stopped_tokens)\n",
    "        if stopped_tokens != []:\n",
    "            ylist.append(stopped_tokens)\n",
    "    #print(ylist)\n",
    "    \n",
    "    # find out the word that are in the vocaburary list with index\n",
    "    para = ''\n",
    "    for i in range(len(ylist)):\n",
    "        find_list = []\n",
    "        for j in range(len(ylist[i])):\n",
    "            if ylist[i][j] in word_dic:\n",
    "                find_list.append(word_dic[ylist[i][j]])\n",
    "        #print(find_list)\n",
    "        #count the occurance of the word in each paragraphy\n",
    "        if find_list != []:\n",
    "            para_list = []\n",
    "            for h in find_list:\n",
    "                if h not in para_list:                \n",
    "                    para = para + h + ':' + (str(find_list.count(h)) + ',')\n",
    "                    para_list.append(h)\n",
    "            para = para[:-1] + '\\n'\n",
    "    #print(para)\n",
    "    f = open(\"./sparse_files/\"+file, \"w+\")\n",
    "    f.write(para)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For task 1, to token the words and create the word list, it is useful to generate the list by using dictionary function like counter.\n",
    "2. For task 2, to make the boundary lists, and for task 3, to make the index and occurance list, it is to create lists and make counting. \n",
    "3. For 3 tasks, it is to use input and output file methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
